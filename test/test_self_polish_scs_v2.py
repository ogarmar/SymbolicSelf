# test/test_self_polish_scs_v2.py â€” Test del MÃ³dulo 1 (Self-Polish + SCS) con imagen
"""
Verifica que SelfPolishCore con entrada MULTIMODAL:
  1. Genera variantes refinadas usando imagen + templates
  2. Calcula SCS real con sÃ­mbolos visuales (no vacÃ­os)
  3. Selecciona la variante con mejor SCS
  4. Compara SCS visual vs SCS text-only
"""

import sys
import io
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import torch
from PIL import Image

from src.model_loader import load_model_sync
from src.symbol_detector import SymbolDetector
from src.m1_self_polish import SelfPolishCore
from test.utils import download_image


IMAGE_URL = "http://images.cocodataset.org/val2017/000000039769.jpg"


def main():
    print("ğŸ”§ Cargando modelo para test visual de Self-Polish + SCS...")

    model, processor = load_model_sync()

    image = download_image(IMAGE_URL)
    print(f"âœ… Imagen descargada: {image.size}")

    detector = SymbolDetector(model)
    core = SelfPolishCore(model, processor.tokenizer, detector)

    # â”€â”€ Test 1: Generar respuesta baseline con imagen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“Š TEST 1: Generar baseline con imagen")
    question = "How many cats are in the image?"
    prompt = f"USER: <image>\n{question} ASSISTANT:"

    inputs = processor(text=prompt, images=image, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        baseline_ids = model.generate(
            **inputs,
            max_new_tokens=30,
            do_sample=False,
            pad_token_id=processor.tokenizer.eos_token_id,
            use_cache=False,  # prevenir fuga de KV-cache en 6GB GPU
        )

    new_tokens = baseline_ids[0][inputs["input_ids"].shape[-1]:]
    baseline_response = processor.decode(new_tokens, skip_special_tokens=True).strip()
    print(f"   Baseline: {baseline_response}")

    # â”€â”€ Test 2: Extraer sÃ­mbolos del baseline (con imagen) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“Š TEST 2: Extraer sÃ­mbolos baseline (multimodal)")
    baseline_symbols, _, var_baseline = detector.extract_symbols(
        inputs["input_ids"],
        pixel_values=inputs.get("pixel_values"),
        image_sizes=inputs.get("image_sizes"),
    )

    n_baseline = len(set(baseline_symbols[baseline_symbols >= 0]))
    print(f"   {n_baseline} sÃ­mbolos, varianza PCA: {var_baseline:.3f}")
    print(f"   DistribuciÃ³n: {baseline_symbols[:20]}...")

    # â”€â”€ Test 3: Generar variantes refinadas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“Š TEST 3: Generar 3 variantes refinadas con imagen")

    # Para las variantes con imagen, procesamos cada template
    templates = [
        f"USER: <image>\n{question} Be precise and count carefully. ASSISTANT:",
        f"USER: <image>\n{question} Look at every detail in the image. ASSISTANT:",
        f"USER: <image>\n{question} Describe what you see clearly. ASSISTANT:",
    ]

    variants = []
    variant_scs_scores = []

    for i, tmpl_prompt in enumerate(templates):
        tmpl_inputs = processor(text=tmpl_prompt, images=image, return_tensors="pt")
        tmpl_inputs = {k: v.to(model.device) for k, v in tmpl_inputs.items()}

        with torch.no_grad():
            var_ids = model.generate(
                **tmpl_inputs,
                max_new_tokens=30,
                do_sample=True,
                temperature=0.7,
                pad_token_id=processor.tokenizer.eos_token_id,
                use_cache=False,  # prevenir fuga de KV-cache en 6GB GPU
            )

        new_ids = var_ids[0][tmpl_inputs["input_ids"].shape[-1]:]
        variant_text = processor.decode(new_ids, skip_special_tokens=True).strip()
        variants.append(variant_text)

        # Extraer sÃ­mbolos de la variante (con imagen)
        var_symbols, _, _ = detector.extract_symbols(
            tmpl_inputs["input_ids"],
            pixel_values=tmpl_inputs.get("pixel_values"),
            image_sizes=tmpl_inputs.get("image_sizes"),
        )

        # Calcular SCS vs baseline
        if len(var_symbols) > 0 and len(baseline_symbols) > 0:
            scs, metrics = detector.compute_scs(var_symbols, baseline_symbols)
        else:
            scs, metrics = 0.0, {}

        variant_scs_scores.append(scs)
        print(f"   V{i+1}: SCS={scs:.3f} | {variant_text[:80]}...")

    # â”€â”€ Test 4: Seleccionar la mejor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“Š TEST 4: Seleccionar mejor variante por SCS")
    best_idx = max(range(len(variant_scs_scores)), key=lambda i: variant_scs_scores[i])
    best_scs = variant_scs_scores[best_idx]
    best_variant = variants[best_idx]

    print(f"   ğŸ† Ganadora: V{best_idx+1} con SCS={best_scs:.3f}")
    print(f"   Texto: {best_variant}")
    print(f"   Baseline: {baseline_response}")

    assert best_scs >= 0.0, f"âŒ SCS negativo: {best_scs}"
    assert len(best_variant) > 0, "âŒ Variante vacÃ­a"
    print("âœ… SelecciÃ³n por SCS completada")

    # â”€â”€ Test 5: Verificar que SCS mejora con variantes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“Š TEST 5: Resumen de scores")
    print(f"   {'Variante':<12} {'SCS':>6}")
    print(f"   {'â”€'*12} {'â”€'*6}")
    for i, scs in enumerate(variant_scs_scores):
        marker = " ğŸ†" if i == best_idx else ""
        print(f"   V{i+1:<10} {scs:>6.3f}{marker}")

    # â”€â”€ Cleanup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    detector.remove_hooks()
    torch.cuda.empty_cache()
    print("\nğŸ‰ Todos los tests visuales pasaron. Self-Polish + SCS V2 OK.")


if __name__ == "__main__":
    main()
